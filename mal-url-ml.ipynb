{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malicious URL Detection With Machine Learning om Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is based on a article on Medium https://medium.com/@ismaelbouarfa/malicious-url-detection-with-machine-learning-d57890443dec <br/>\n",
    "The purpose of this workshop is to classify URLs given as inputs to predict if they are dangerous or inoffensive.\n",
    "We selected **good** as a label for the legitimate ones and **bad** for the malicious. Using a dataset with many URLs (as text) already labeled, located in a CSV file, we’ll train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a URL<br>\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*EaleKgM4uK8sCpvy)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by following the original post by using SKLearn and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please replace **ENTER YOUR BUCKET NAME HERE** with your bucket name that you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "bucket = \"ENTER YOUR BUCKET NAME HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we copy over our datasets from a public S3 bucket. The GIT Repo for this dataset has been commented out and provided for your information <br>\n",
    "Also an additional dataset called gdms_data.csv is present and contains additional urls and their statuses to augment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Source dataset \n",
    "#! git clone https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs.git\n",
    "! aws s3 cp s3://ml-materials/malicious-url-dataset/data.csv .\n",
    "! aws s3 cp s3://ml-materials/malicious-url-dataset/gdms_data.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the datasets into dataframes, sanitize the datasets and then create a new column that maps the \"good\" and \"bad\" classes to 0 and 1<br>\n",
    "The source dataset contains 60K urls and statuses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data exploration and sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets into dataframes\n",
    "df = pd.read_csv('./data.csv',',',error_bad_lines=False)\n",
    "df = pd.DataFrame(df)\n",
    "df2 = pd.read_csv('./gdms_data.csv',',',error_bad_lines=False)\n",
    "df2 = pd.DataFrame(df2)\n",
    "\n",
    "# Create a random sample set of 10000 rows\n",
    "df = df.sample(n=10000)\n",
    "\n",
    "# Append the custom dataset to the training dataset\n",
    "df = df.append(df2)\n",
    "\n",
    "from io import StringIO\n",
    "col = ['label','url']\n",
    "df = df[col]\n",
    "\n",
    "#Deleting nulls\n",
    "df = df[pd.notnull(df['url'])]\n",
    "\n",
    "#more settings for our data manipulation\n",
    "df.columns = ['label', 'url']\n",
    "df['category_id'] = df['label'].factorize()[0]\n",
    "category_id_df = df[['label', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'label']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets take a look at our resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a dataset composed of good and bad urls. Now, how can we detect if we have more good urls in the dataset? <br>This code allows you to create a simple bar chart of class proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "BAD_len = df[df['label'] == 'bad'].shape[0]\n",
    "GOOD_len = df[df['label'] == 'good'].shape[0]\n",
    "plt.bar(10,BAD_len,3, label=\"BAD URL\")\n",
    "plt.bar(15,GOOD_len,3, label=\"GOOD URL\")\n",
    "plt.legend()\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Proportion of examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more good URLs. That’s what we thought. We can learn more from the good URLs to avoid bias or generalize words present in bad URL and false positives then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Selection and applying Natural language processing\n",
    "Now that we have our dataset we’ll choose an approach:<br><br>\n",
    "What is NLP? It is the science of programming computers to process and analyze natural language. Some large and mature applications are speech recognition, translation, text classification, and so on. As the URLs are full of words (domaine name, path, file, extension…) we wanted to try it!<br><br>\n",
    "As learning algorithms work with numerical features we need to convert words into numerical vectors. How does it works? The first entry technique is called “Bag Of Words”.<br><br>\n",
    "In Bag of Words a good classifier detects patterns in words distribution and which words occur and how many times for each kind of text. However, words count is not always the best idea. As demonstrated previously, a URL can be very large and add bias for our predictions.\n",
    "The following code allows you to represent for each different length the number of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "lens = df.url.str.len()\n",
    "lens.hist(bins = np.arange(0,300,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we are going to define a function to tokenize the URL strings so we can generate feature vectors.<br>\n",
    "Note this function also removes common urls comntaining **.com** as it would be too prevalent and might skew the predictions from our model<br>\n",
    "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer function for URL by Faizan Ahmad, CEO FSecurify\n",
    "def getTokens(input):\n",
    "    tokensBySlash = str(input.encode('utf-8')).split('/')\n",
    "    allTokens=[]\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        tokensByDot = []\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\n",
    "            tokentsByDot = tokensByDot + tempTokens\n",
    "        allTokens = allTokens + tokens + tokensByDot\n",
    "    allTokens = list(set(allTokens))\n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to avoid Bags of Words and use another technique. So improving our model can be done using Term Frequency (TF) and Inverse Document Frequency (IDF):<br>\n",
    "![](https://miro.medium.com/max/1050/1*V01pYGwqnMlWiwP26NjbZQ.png)<br>\n",
    "This technique downscales the influence of some respecting words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = [d[1]for d in df] #labels\n",
    "#myUrls = [d[0]for d in df] #urls\n",
    "vectorizer = TfidfVectorizer( tokenizer=getTokens ,use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "features = vectorizer.fit_transform(df.url).toarray().astype('float32')\n",
    "labels = df.label\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Training\n",
    "Logistic regression is used to model the probability of a certain class. Nowadays it is used in various fields, predicting risks of developing a disease based on test results, to classify if an image contains a rose or a cactus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = LogisticRegression(random_state=0)\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.20, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "clf = LogisticRegression(random_state=0) \n",
    "clf.fit(X_train,y_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print ('train accuracy =', train_score)\n",
    "print ('test accuracy =', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we divided our dataset in a train test used to fit the parameters and feed our model. Then with the test set we validated our model with an unbiased evaluation. The scores are good:<br><br>\n",
    "train accuracy = 0.843375<br><br>\n",
    "Based on the real class of the labels and the predicted class, we can introduce the following concepts: Type I error & Type II error (from the confusion matrix). As you can imagine, when classifying images on one category or another, it’s possible to have two types of errors:<br><br>\n",
    "The type I error occurs when something is true but rejected, also known as false positive or false alarm.<br>\n",
    "The type II error occurs when something is false but accepted, it’s a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    " xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, approx 16K bad URL have been correctly predicted as malicious URLs. However approx 300 legitimate URLs have been identified as malicious. These “false alarms” are known as Type I error. This kind of errors are one example of reason why monitoring and supervision are necessary when applying machine learning to cybersecurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Predictions\n",
    "And then we made our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = ['yahoo.fr','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html','https://mail.google.com/','https://gmail.com']\n",
    "X_feature_vectors = vectorizer.transform(X_predict)\n",
    "y_Predict = clf.predict(X_feature_vectors)\n",
    "print(y_Predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Steps 2 through 4 in Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically we do not really need to do the next two cells as we already did it earlier. These have been left in to show a more complete example in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we use the TfidfVectorizer to create the feature vectors for the URL's  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( tokenizer=getTokens ,use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "labels = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnsfeat = vectorizer.fit_transform(df.url).toarray().astype('float32')\n",
    "gen_labels = labels.replace({'good': 0, 'bad': 1}).values.astype('float32')\n",
    "dnsfeat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the variable gen_labels you can see its very similar to the category_id that we calculated earlier. We leave it as an exercise to replace gen_labels with category_id from the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnsfeat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the libraries for SageMaker and boto3 for the AWS Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the training file and storing it to your bucket in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "prefix = \"sagemaker/mal-url\"\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, dnsfeat, gen_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'linearlearner'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use LinearLearner (a SageMaker Built-in Algorithm) to be able to do a similar Logistic Regression that was used earlier. This algorithm requires the number of features which we calculate next  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(dnsfeat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the latest container available for the LinearLearner algorithm from our region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "container = image_uris.retrieve(region=boto3.Session().region_name, framework=\"linear-learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the hyperparameters for this algorithm, the output location for our model artifacts and start the training job. <br>\n",
    "This cell takes approx 20 minutes to run<br>\n",
    "NOTE other alternate values have been provided and commented out for you to test later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "linear.set_hyperparameters(predictor_type=\"binary_classifier\",epochs=200,mini_batch_size=100,loss=\"logistic\",optimizer=\"sgd\") \n",
    "# Other Hyperparameters to try\n",
    "# mini_batch_size=200\n",
    "# mini_batch_size=1000\n",
    "# remove optimizer=\"sgd\"\n",
    "\n",
    "linear.fit({\"train\": s3_train_data},logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training job is finished, lets deploy this model to an endpoint to be able to perform real time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup our endpoint serializers and deserializers. We are telling the endpoint to expect CSV format for input and output the. results as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "linear_predictor.serializer = CSVSerializer()\n",
    "linear_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a single prediction from one of the rows in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_predictor.predict(dnsfeat[30:31], initial_args={\"ContentType\": \"text/csv\"})\n",
    "print(result)\n",
    "print(gen_labels[30:31])\n",
    "print(labels[30:31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do some predictions on some new url's.<br>\n",
    "Notice we take our test URL's and first convert it to a feature vector form and provide it to our endpoint to make the prediction.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = ['yahoo.fr','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html','https://mail.google.com/','https://gmail.com']\n",
    "X_feature_vectors = vectorizer.transform(X_predict).toarray()\n",
    "result = linear_predictor.predict(X_feature_vectors, initial_args={\"ContentType\": \"text/csv\"})\n",
    "for i in range(len(X_predict)):\n",
    "    res = \"good\" if result['predictions'][i]['predicted_label']==0 else \"bad\"\n",
    "    print(f'{X_predict[i]} - {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delete our endpoint, lets see how this would in a practical application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this prediction example works great in a notebook, however how would you use this vectorizer in your own scripts to call the endpoint when running outside of this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is that we save the vectorizer model artifacts using the joblib format. In your prediction script/code you would then load the last generated vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load the joblib library (joblib is a serialization library for models. An example of another library is pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets save the vectorizer model to tfidf1.joblib. This could easily be uploaded to S3 although we are leaving on this notebook instance for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer, open(\"tfidf1.joblib\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the vectorizer model and use it to create the feature vectors from our test URL's to send to our endpoint to make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec = joblib.load('tfidf1.joblib')\n",
    "New_X_test = ['yahoo.fr','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html']\n",
    "New_X_test_vectorized = vectorizer.transform(New_X_test).toarray()\n",
    "result = linear_predictor.predict(New_X_test_vectorized, initial_args={\"ContentType\": \"text/csv\"})\n",
    "for i in range(len(New_X_test)):\n",
    "    res = \"good\" if result['predictions'][i]['predicted_label']==0 else \"bad\"\n",
    "    print(f'{X_predict[i]} - {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we end this workshop, lets generate a confusion matrix of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "new_y_pred = []\n",
    "for i in range(0,len(X_test),50):\n",
    "    tmpout=linear_predictor.predict(X_test[i:i+50], initial_args={\"ContentType\": \"text/csv\"})\n",
    "    y_pred_list=y_pred_list + tmpout['predictions']\n",
    "for vals in y_pred_list:\n",
    "    new_y_pred.append(vals['predicted_label'])\n",
    "y_pred = np.asarray(new_y_pred, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    " xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "print ('test accuracy =', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets go ahead and delete our endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
