{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malicious URL Detection With Machine Learning on Amazon SageMaker and Offline Predictions\n",
    "\n",
    "This workshop is based on a article on Medium https://medium.com/@ismaelbouarfa/malicious-url-detection-with-machine-learning-d57890443dec <br/>\n",
    "The purpose of this workshop is to classify URLs given as inputs to predict if they are dangerous or inoffensive.\n",
    "We selected **good** as a label for the legitimate ones and **bad** for the malicious. Using a dataset with many URLs (as text) already labeled, located in a CSV file, weâ€™ll train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a URL<br>\n",
    "\n",
    "![](https://miro.medium.com/max/1050/0*EaleKgM4uK8sCpvy)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please replace **ENTER YOUR BUCKET NAME HERE** with your bucket name that you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "bucket = \"ENTER YOUR BUCKET NAME HERE\"\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "print(\"Using bucket \" + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Now we copy over our datasets from a public S3 bucket. The GIT Repo for this dataset has been commented out and provided for your information\n",
    "Also an additional dataset called gdms_data.csv is present and contains additional urls and their statuses to augment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Source dataset \n",
    "#! git clone https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs.git\n",
    "! aws s3 cp s3://ml-materials/malicious-url-dataset/data.csv .\n",
    "! aws s3 cp s3://ml-materials/malicious-url-dataset/gdms_data.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets into dataframes\n",
    "df = pd.read_csv('./data.csv',',',error_bad_lines=False)\n",
    "df = pd.DataFrame(df)\n",
    "df2 = pd.read_csv('./gdms_data.csv',',',error_bad_lines=False)\n",
    "df2 = pd.DataFrame(df2)\n",
    "\n",
    "# Create a random sample set of 10000 rows\n",
    "df = df.sample(n=10000)\n",
    "\n",
    "# Append the custom dataset to the training dataset\n",
    "df = df.append(df2)\n",
    "\n",
    "from io import StringIO\n",
    "col = ['label','url']\n",
    "df = df[col]\n",
    "\n",
    "#Deleting nulls\n",
    "df = df[pd.notnull(df['url'])]\n",
    "\n",
    "#more settings for our data manipulation\n",
    "df.columns = ['label', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer function for URL by Faizan Ahmad, CEO FSecurify\n",
    "def getTokens(input):\n",
    "    tokensBySlash = str(input.encode('utf-8')).split('/')\n",
    "    allTokens=[]\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        tokensByDot = []\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\n",
    "            tokentsByDot = tokensByDot + tempTokens\n",
    "        allTokens = allTokens + tokens + tokensByDot\n",
    "    allTokens = list(set(allTokens))\n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\n",
    "    return allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( tokenizer=getTokens ,use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "features = vectorizer.fit_transform(df.url).toarray().astype('float32')\n",
    "labels = df.label\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point since we have the vectorizer, we need to serialize it at store it locally. We will be using Joblib to serialize the vectorizer model (we could have also used Pickle). Then we will copy it to the S3 bucket so we can use it at the end of this workshop for doing inference outside of the AWS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(vectorizer, open(\"tfidf1.joblib\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp tfidf1.joblib s3://{bucket}/models/vectorizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the dataset into 80/20 split for training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset split, we can assemable the features and labels for training and testing datasets and save as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd_df = pd.DataFrame(data = X_train, \n",
    "                        columns = ['Column_' + str(i + 1) \n",
    "                        for i in range(X_train.shape[1])])\n",
    "\n",
    "train_pd_df['LABEL'] = y_train.values\n",
    "\n",
    "test_pd_df = pd.DataFrame(data = X_test, \n",
    "                        columns = ['Column_' + str(i + 1) \n",
    "                        for i in range(X_test.shape[1])])\n",
    "\n",
    "test_pd_df['LABEL'] = y_test.values\n",
    "\n",
    "train_pd_df.to_csv(\"mal_url_train.csv\", index=False)\n",
    "test_pd_df.to_csv(\"mal_url_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now copy the CSV files to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"mal_url_train.csv\", bucket=bucket, key_prefix=\"sagemaker/sklearncontainer\"\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"mal_url_test.csv\", bucket=bucket, key_prefix=\"sagemaker/sklearncontainer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a *Script Mode* script\n",
    "The below script contains both training functionality and can run both in SageMaker Training hardware or locally (desktop, SageMaker notebook, on prem, etc). Detailed guidance here https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#preparing-the-scikit-learn-training-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    prediction = model.predict(input_data)\n",
    "    pred_prob = model.predict_proba(input_data)\n",
    "    return np.array([prediction, pred_prob[:,0], pred_prob[:,1]])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument(\"--random_state\", type=int, default=0)\n",
    "    #parser.add_argument(\"--target\", type=str, default=\"\")\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"mal_url_train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"mal_url_test.csv\")\n",
    "    parser.add_argument(\n",
    "        \"--features\", type=str\n",
    "    )  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str\n",
    "    )  # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"reading data\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print(\"building training and testing datasets\")\n",
    "    X_train = train_df.drop(args.target,1)\n",
    "    X_test = test_df.drop(args.target,1)\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print(\"training model\")\n",
    "    \n",
    "    model = LogisticRegression(random_state=args.random_state)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"validating model\")\n",
    "    \n",
    "    train_score = model.score(X_train, y_train)\n",
    "    print(f\"Training Accuracy = {train_score}\")\n",
    "    \n",
    "    test_score = model.score(X_test, y_test)\n",
    "    print(f\"Test Accuracy = {test_score}\")\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"model persisted at \" + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching a training job with the Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the training job to use SKLearn pre-built container and the script for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.2xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"mal-url-scikit\",\n",
    "    hyperparameters={\n",
    "        \"target\": \"LABEL\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything setup, we can now kick off the training job providing the locations for the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training has successfully completed, let us deploy to an SageMaker endpoint to be able to do realtime predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn_estimator.deploy(instance_type=\"ml.m4.2xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint is up and running. We can now do predictions against this endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = ['yahoo.fr','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html','https://mail.google.com/','https://gmail.com']\n",
    "X_feature_vectors = vectorizer.transform(X_predict).toarray()\n",
    "y_Predict = predictor.predict(X_feature_vectors)\n",
    "x = len(y_Predict[1,:])\n",
    "for y  in range(0,x):\n",
    "    print(y_Predict[:, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Predictions from a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and using the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do predictions outside of AWS, we will start by installing the joblib library as we will need it to deserialze the vectorizer model we saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now copy the vectorizer model and the trained SKLearn model from your S3 bucket to your local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uri = f\"s3://{bucket}/models/vectorizer/tfidf1.joblib\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! aws s3 cp {vectorizer_uri} tfidf1.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the vectorizer model file local, we can run the load and run vectorizer on our test urls<br>\n",
    "We will load and use joblib to load the vectorizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "local_test_urls = ['gmail.com','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html']\n",
    "\n",
    "local_vectorizer = joblib.load('tfidf1.joblib')\n",
    "\n",
    "test_urls_vectorized = local_vectorizer.transform(local_test_urls).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting predictions from a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install boto3 if not already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_runtime_client = boto3.client('sagemaker-runtime',\n",
    "                #        aws_access_key_id='<your_access_key_id>', \n",
    "                #        aws_secret_access_key='<your_secret_access_key>',\n",
    "                #        region_name='us-east-1',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = json.dumps(test_urls_vectorized.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace \"PUT YOUR ENDPOINT NAME HERE\" with your endpoint name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"PUT YOUR ENDPOINT NAME HERE\"\n",
    "\n",
    "response = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    Body = payload,\n",
    "    ContentType='application/json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(result[0])\n",
    "for y  in range(0,x):\n",
    "    print(result[0][y],result[1][y],result[2][y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting predictions directly from a model without using endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and using the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do predictions outside of AWS, we will start by installing the joblib library as we will need it to deserialze the vectorizer model we saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now copy the vectorizer model and the trained SKLearn model from your S3 bucket to your local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uri = f\"s3://{bucket}/models/vectorizer/tfidf1.joblib\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! aws s3 cp {vectorizer_uri} tfidf1.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the vectorizer model file local, we can run the load and run vectorizer on our test urls<br>\n",
    "We will load and use joblib to load the vectorizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "local_test_urls = ['gmail.com','www.radsport-voggel.de/wp-admin/includes/log.exe','hello.ru','auction-korea.co.kr/technote7/peace/','https://www.kdnuggets.com/2016/10/machine-learning-detect-malicious-urls.html']\n",
    "\n",
    "local_vectorizer = joblib.load('tfidf1.joblib')\n",
    "\n",
    "test_urls_vectorized = local_vectorizer.transform(local_test_urls).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and using the SKLearn Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to copy the trained SKLearn model file to our local filesystem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"{sklearn_estimator.output_path}{sklearn_estimator._current_job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {model_uri} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now uncompress and untar the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar zxvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the vectorized test urls from Step (1) which we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the trained model for the Scikit Learn Logistic Regression and run predictions and confidence of our vectorized urls array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the saved model with joblib\n",
    "local_model = joblib.load('model.joblib')\n",
    "\n",
    "# apply the whole pipeline to data\n",
    "pred = pd.Series(local_model.predict(test_urls_vectorized))\n",
    "prob = local_model.predict_proba(test_urls_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the predictions and associated confidence ratings of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(pred)\n",
    "for y  in range(0,x):\n",
    "    print(pred[y],prob[y][0],prob[y][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget to delete the endpoint !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
